<h1 id="neural-networks-and-gaussian-processes-neural-tangent-kernel">Neural Networks and Gaussian Processes. Neural Tangent Kernel</h1>

<p>Around a month ago I set out to learn about the connection between Neural Networks (NN) and Gaussian Processes (GP), something that had been appearing more and more in the seminars I had been attending and literature I had been reading at the time.</p>

<p>This connection is <em>theoretically</em> interesting because of the following. Standard NNs belong to the classical parametric frequentist approach to statistical learning. On the other hand, GP is a nonparametric modeling technique that belongs to the realm of Bayesian inference. On the <em>practical</em> side, GPs are classical objects that have been understood much better than deep NNs plus this connection sheds light on the training dynamics of a NN as I will explain below.</p>

<h2 id="refresher-on-gaussian-processes">Refresher on Gaussian Processes</h2>

<p>Every parametric model with $n$ inputs and $m$ outputs defines a mapping $F_{\theta}: \mathbb{R}^n \to \mathbb{R}^m$</p>
